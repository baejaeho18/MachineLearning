{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Mount your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FGrw6kiveUAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71395294-533d-4c3e-8afd-29bb14f41aa3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n",
        "1.   Implement and apply kNN on MNIST with k=1, 5, 10. Apply kNN on raw images, and 2, 7 dimensional eigenspaces, respectively. Show the accuracy scores for each run (you'd run the algorithm 9 times).\n",
        "\n",
        "2.   Use and run the Random Forest algorithm for MNIST classification (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Show the accuracy scores.\n",
        "\n",
        "If it takes too much time, reduce the number of samples for training/testing (based on random selection)."
      ],
      "metadata": {
        "id": "XtZTSiDRAp2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code here\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import urllib.request\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "# from Homework1\n",
        "def load_data(dataset):\n",
        "  if not os.path.isfile(dataset):\n",
        "    origin = (\n",
        "      'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
        "    )\n",
        "    print('Downloading data from %s' % origin)\n",
        "    urllib.request.urlretrieve(origin, dataset)\n",
        "  print('Loading data...')\n",
        "\n",
        "  with gzip.open(dataset, 'rb') as f:\n",
        "    try:\n",
        "      train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "    except:\n",
        "      train_set, valid_set, test_set = pickle.load(f)\n",
        "  print('... data has been loaded!')\n",
        "\n",
        "  return train_set, valid_set, test_set\n",
        "\n",
        "def KNN_classifier(X_train, y_train, X_test, n_neighbors):\n",
        "  y_pred = []\n",
        "  for x in X_test:\n",
        "    distances = [np.linalg.norm(x - x_train) for x_train in X_train]\n",
        "    k_indices = np.argsort(distances)[:n_neighbors]\n",
        "    k_nearest_labels = [y_train[i] for i in k_indices]\n",
        "    # Count occurrences of each class label\n",
        "    label_counts = np.bincount(k_nearest_labels)\n",
        "    most_common_labels = np.where(label_counts == label_counts.max())[0]\n",
        "    if len(most_common_labels) == 1:\n",
        "      y_pred.append(most_common_labels[0])\n",
        "    else:\n",
        "      # If there's a tie, return the label with the smallest distance\n",
        "      tie_distances = [distances[i] for i in k_indices if np.isin(y_train[i], most_common_labels)]\n",
        "      if not tie_distances:\n",
        "        min_distance_index = np.argmin(distances)\n",
        "        y_pred.append(y_train[min_distance_index])\n",
        "      else:\n",
        "        y_pred.append(most_common_labels[np.argmin(tie_distances)])\n",
        "  return np.array(y_pred)\n",
        "\n",
        "def pca(X, n_components):\n",
        "  mean = np.mean(X, axis=0)\n",
        "  centered_data = X - mean\n",
        "  cov_matrix = np.cov(centered_data, rowvar=False)\n",
        "  eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "  sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "  eigenvectors = eigenvectors[:, sorted_indices]\n",
        "  selected_eigenvectors = eigenvectors[:, :n_components]\n",
        "  reduced_data = centered_data.dot(selected_eigenvectors)\n",
        "  return reduced_data\n",
        "\n",
        "# Process data\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "  correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n",
        "  total = len(y_true)\n",
        "  accuracy = correct / total\n",
        "  return accuracy\n",
        "\n",
        "# 1) Load the MNIST dataset as in Homework7\n",
        "dataset_file = 'mnist.pkl.gz'\n",
        "train_set, _, test_set = load_data(dataset_file)\n",
        "X_train, y_train = train_set # data, label\n",
        "X_test, y_test = test_set\n",
        "\n",
        "# # Select a subset of the data to speed up the computation (optional)\n",
        "subset_size = 1000\n",
        "X_train = X_train[:subset_size]\n",
        "y_train = y_train[:subset_size]\n",
        "X_text = X_test[:subset_size]\n",
        "y_text = y_test[:subset_size]\n",
        "\n",
        "# kNN on Raw Images\n",
        "k_values = [1, 5, 10]\n",
        "for k_value in k_values:\n",
        "  y_pred = KNN_classifier(X_train, y_train, X_test, k_value)\n",
        "  accuracy = calculate_accuracy(y_test, y_pred)\n",
        "  print(f\"Accuracy(k={k_value}) : {accuracy}\")\n",
        "\n",
        "# kNN on  Reduced-Dementional Spaces\n",
        "reduced_dimensions = [2, 7]\n",
        "for reduced_dimension in reduced_dimensions:\n",
        "  X_train_reduced = pca(X_train, reduced_dimension)\n",
        "  X_test_reduced = pca(X_test, reduced_dimension)\n",
        "\n",
        "  for k_value in k_values:\n",
        "    y_pred = KNN_classifier(X_train_reduced, y_train, X_test_reduced, k_value)\n",
        "    accuracy = calculate_accuracy(y_test, y_pred)\n",
        "    print(f\"Accuracy(k={k_value}, dim={reduced_dimension}) : {accuracy}\")\n",
        "\n",
        "# Random Forest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy(Random Forest) : {accuracy}\")"
      ],
      "metadata": {
        "id": "J5cik59AByis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48224d2f-019e-4bd2-ed04-cf407ded5cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "... data has been loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT prompts\n",
        "\n",
        "Now solve the same task with ChatGPT with the following prompts:\n",
        "\n",
        "\n",
        "1.   Can you implement and apply kNN on MNIST (from scratch without using any specialized library) with k=1, 5, 10? Can you show the accuracies of the predictions? Ignore the computational cost of this code.\n",
        "2.   Can you use the same provided knn algorithm and apply it to the MNIST data with reduced dimensions to 2 and 7, with k=1, 5, 10? Please provide the accuracies.\n",
        "3.   Can you provide a code for the random forest algorithm applied to the MNIST dataset? Please use specialized libraries like sklearn. Please show the accuracies scores.\n",
        "\n",
        "\n",
        "**Paste the answer provided and run the code below:**"
      ],
      "metadata": {
        "id": "6idIZhGPB1wZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#paste code here\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = fetch_openml(\"mnist_784\")\n",
        "X = mnist.data\n",
        "y = mnist.target\n",
        "\n",
        "# Preprocess data\n",
        "X = X / 255.0  # Normalize data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# kNN on Raw Images\n",
        "k_values = [1, 5, 10]\n",
        "for k in k_values:\n",
        "    kNN_classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "    kNN_classifier.fit(X_train, y_train)\n",
        "    y_pred = kNN_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'kNN (k={k}) on Raw Images Accuracy: {accuracy}')\n",
        "\n",
        "# kNN on Reduced-Dimensional Spaces\n",
        "dimensions = [2, 7]\n",
        "for dim in dimensions:\n",
        "    pca = PCA(n_components=dim)\n",
        "    X_train_reduced = pca.fit_transform(X_train)\n",
        "    X_test_reduced = pca.transform(X_test)\n",
        "\n",
        "    for k in k_values:\n",
        "        kNN_classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "        kNN_classifier.fit(X_train_reduced, y_train)\n",
        "        y_pred = kNN_classifier.predict(X_test_reduced)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f'kNN (k={k}) on {dim}-D Eigenspace Accuracy: {accuracy}')\n",
        "\n",
        "# Random Forest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Random Forest Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "yNdFVuAzBz-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report\n",
        "\n",
        "\n",
        "1.   What are some differences you notice between your code and ChatGPT's? (Name at least 3)\n",
        "\n",
        "1) Data Sourc\n",
        " In my code, the MNIST dataset is loaded from an external source using a URL (http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz).\n",
        "But GPT suggest the MNIST dataset using scikit-learn's 'fetch_openml' function.\n",
        "\n",
        "2) My code was executed by cutting only 1000 data into subsets to speed up calculation performance. On the other hand, GPT used the entire dataset even if it was a different source dataset. Maybe the difference in accuracy comes from here.\n",
        "\n",
        "3) sklearn libirary\n",
        " My code includes a custom PCA (Principal Component Analysis) function and KNN_classifier function. However, GPT uses scikit-learn's 'PCA' module for dimensionality reduction, 'KNeighborsClassifier' module for KNN classifier and 'accuracy_score' module for metric calculation. But, yes, the GPT codes looks really simple and easy.\n",
        "\n",
        "---\n",
        "\n",
        "2.   Ask ChatGPT to compare kNN and Random Forest. Paste the answer here. Do you agree with ChatGPT's statement? Why yes or why not?\n",
        "\n",
        ">> GPT Response:\n",
        "Certainly! K-Nearest Neighbors (k-NN) and Random Forest are both machine learning algorithms, but they have different approaches and characteristics. Let's compare them in various aspects:\n",
        "1. **Algorithm Type**:\n",
        "   - **k-NN**: k-NN is a simple instance-based learning algorithm. It classifies data points based on their similarity to nearby neighbors.\n",
        "   - **Random Forest**: Random Forest is an ensemble learning method that combines multiple decision trees to make predictions.\n",
        "2. **Supervised Learning**:\n",
        "   - **k-NN**: k-NN is a supervised learning algorithm used for both classification and regression tasks.\n",
        "   - **Random Forest**: Random Forest is primarily used for supervised classification and regression tasks.\n",
        "3. **Model Complexity**:\n",
        "   - **k-NN**: K-NN is a simple algorithm with no model training phase. It stores the entire training dataset and calculates distances during prediction.\n",
        "   - **Random Forest**: Random Forest is more complex as it involves training multiple decision trees and combining their predictions.\n",
        "4. **Training**:\n",
        "   - **k-NN**: There is no explicit training in k-NN. The model stores the training data and uses it for prediction.\n",
        "   - **Random Forest**: Random Forest trains an ensemble of decision trees using bootstrapped subsets of the data and random feature selection.\n",
        "5. **Predictions**:\n",
        "   - **k-NN**: k-NN predicts a data point's class based on the majority class among its k-nearest neighbors.\n",
        "   - **Random Forest**: Random Forest combines the predictions of multiple decision trees and typically selects the majority class for classification tasks.\n",
        "6. **Hyperparameters**:\n",
        "   - **k-NN**: The main hyperparameter is 'k,' the number of neighbors to consider. The choice of 'k' impacts the model's bias-variance trade-off.\n",
        "   - **Random Forest**: Random Forest has hyperparameters such as the number of trees, maximum depth of trees, and feature selection criteria.\n",
        "7. **Performance**:\n",
        "   - **k-NN**: k-NN is simple and can perform well on certain datasets, especially when the decision boundary is not too complex. However, it can be sensitive to the choice of 'k.'\n",
        "   - **Random Forest**: Random Forest is known for its strong performance due to its ability to handle complex datasets, reduce overfitting, and provide feature importances.\n",
        "8. **Scaling**:\n",
        "   - **k-NN**: k-NN can be sensitive to the scale of features, so feature scaling is important.\n",
        "   - **Random Forest**: Random Forest is less sensitive to feature scaling.\n",
        "9. **Interpretability**:\n",
        "   - **k-NN**: k-NN is relatively straightforward to interpret because it classifies based on the nearest neighbors.\n",
        "   - **Random Forest**: Random Forest can be less interpretable due to the ensemble nature, but feature importances can be analyzed.\n",
        "10. **Use Cases**:\n",
        "    - **k-NN**: Suitable for simple classification and regression tasks where data has clear clusters or patterns.\n",
        "    - **Random Forest**: Effective for a wide range of classification and regression tasks, including those with complex, high-dimensional data.<br><br>\n",
        "In summary, the choice between k-NN and Random Forest depends on the nature of your data, the complexity of the problem, and your specific goals. k-NN is simple and can work well on certain datasets, while Random Forest is a versatile and powerful algorithm that often delivers robust performance in a variety of scenarios.\n",
        "\n",
        "Yes, it talks well. I can summary and add some explanation.\n",
        "k-NN is easy to understand and implement. It can work well when the decision boundary is smooth and data has clear clusters. But calculating distances to all training examples can be slow, especially with large datasets.\n",
        "Random Forest typically provides robust and accurate results due to combining multiple decision trees. So it's suitable for high-dimensional and complex decision boundaries data.\n",
        "In conclusion, Random Forest is a more versatile and powerful algorithm, making it a common choice, but k-NN can still be valuable in certain situations where simplicity and ease of implementation are important.\n",
        "\n",
        "---\n",
        "\n",
        "3.   In yours and ChatGPTs implementation, which algorithm had the highest accuracy?\n",
        "\n",
        "In the result of both my code and ChatGPT's code, the Random Forest algorithm had the highest accuracy.\n",
        "- My Implementation : 0.8791\n",
        "- ChatGPT's Implementation : 0.9675\n",
        "\n",
        "---\n",
        "\n",
        "4.   Conclude with some thoughts and things you learned from this homework.\n",
        "\n",
        "I experienced the implementations for solving a classification problem on the MNIST dataset using k-NN and Random Forest algorithms.\n",
        "While the custom k-NN implementation in my code worked, it was less efficient and potentially less accurate than scikit-learn's KNeighborsClassifier. The accuracy results shows the Random Forest algorithm is better than KNN algorithm in both implementations.\n",
        "Custom implementations was really educational in the aspect of studying mathmetical concepts, but using well-optimized libraries like scikit-learn must be happy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TZPU5Iz9C2IS"
      }
    }
  ]
}